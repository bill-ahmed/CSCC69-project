            +--------------------+
            | CSCC69             |
            | PROJECT 1: THREADS |
            | DESIGN DOCUMENT    |
            +--------------------+
   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Bilal Ahmed <bill.ahmed@mail.utoronto.ca>
Tanner Bergeron tanner.bergeron@mail.utoronto.ca
Tanaan Karunakaran tanaan.karunakaran@mail.utoronto.ca

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

    /* Required a notion of a thread being asleep since blocked is
    going to be reserved for locks. */
    enum thread_status
    {
        /* New thread state */
        THREAD_SLEEPING,    /* Waiting on timer to wake it up */ 
    };

    /* If threads can sleep, we need to know for how long. Member only
    accessed if thread is running and going to sleep */
    struct thread
    {
        /* New struct member */
        int64_t wakeup_time;     /* When to wake up this thread from sleep */
    }

    /* Required to keep track of which threads are currently sleeping.
    Sorted by thread->wakeup_time ASC for fast timer checks */
    struct list sleeped_threads_list;

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

    timer_sleep calls thread_sleep with the time it should wake
    up at.

    thread_sleep fills in the thread information, sets it's status
    to sleeping, adds it to a sleeping thread list, and finally
    calls the scheduler to switch threads. All of these actions
    must be atomic, so interrupts are disabled for the duration
    of these calls inside thread_sleep.

    Once the stack is restored and it wakes up, it continues and
    returns from timer_sleep as it should.

    When a timer interrupt occurs, it ticks and then it checks
    if any sleeping threads should be woken up. Any threads that
    have waited long enough are dequeued from the sleeping list,
    made their status set to ready, and added to the ready list.

    This means when the timer interrupt finishes and yields, a
    newly awoken thread now has the possibility of being chosen
    and run.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

    The list of sleeping threads is sorted by wakeup_time which
    means if we know the current time, we can look at the first
    thread in the list and if it is not ready to be awoken, we
    also know none of the others are either. This means sleeping
    a large number of threads causes no penalty to timer interrupts.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

    A thread is only able to put itself to sleep, therefore until
    it runs code to actually change the state of the thread and
    add it to the sleeping list, and schedule, all other operations
    need not be atomic. The few that are, are surrounded by interrupt
    disable and enable.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

    Atomic section is minimally wrapped by interrupt disable/restore.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

    We needed a way to tell the scheduler not to schedule a sleeping
    thread, and a way to wake it up efficiently after an amount of time.
    Busy waiting will not do.

    We wanted to write a minimal amount of code because adding something
    overly complex just introduces bugs. Having a sorted list of sleeping
    threads was the most efficient because of what is mentioned in A3.
    Each timer interrupt we need to check if we should wake up any sleeping
    threads. That iteration through the list of potentially hundreds of
    threads is costly and sorting by wake up time allows us to often only
    need to check the first thread in the list to decide whether to continue.

    This was an efficient implementation with not much code added to the
    base handout.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

    struct thread
        {
            int priority_donated;               /* Priority temporarily donated to this thread from another. */
            struct list locks_holding;          /* List of all the locks held by this thread */
            struct lock *lock_waiting_on;       /* A lock this thread is waiting on */
        }

    struct lock
        {
            struct list_elem elem;      /* Which thread this lock is being held by */
        }

    struct semaphore_elem 
        {
            int priority;                       /* Priority. */
        }

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

    To track priority donations, a pointer to a lock is kept with each thread indicating which lock it is waiting on.
    Since each lock has a holder, the thread is able to access the holder of the lock and donate its priority when 
    acquiring said lock.

    Diagram:
    --------
    Please look at "A1 - Threads.png" which is in the same directory as this file.

    In each step of the diagram, the previous thread is waiting on the next thread. Assuming priority goes in order of:
    T_a > T_b > T_c, Thread A will donate to Thread B, and so forth. In the event that Thread B is in the ready queue, 
    we also keep track of the waiters for each thread via the semaphore embedded in each lock. This lets us account for the
    case where a new thread, Thread D for priority of T_d > T_a, is waiting on Thread B, then T_d's priority can be donated 
    to B, and then C and so forth.

    Further, whenever a thread releases a lock, it's priority is reverted back to the original IF there are no other locks 
    it is holding. In the case where T_b was holding two or more locks, then its priority would be re-calculated based on 
    ALL the remaining locks it is holding. This helps ensure that T_b is given the correct order in ready_list, hence why 
    we added the list `locks_holding` to each thread.


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

    The list of thread waiting for a semaphore (inside the list "waiters") is kept sorted by
    priority at all times. This ensures that when we pop from the front during a sema_up 
    operation, the highest priority thread is woken up. Since this works for semaphore, the 
    functionality is also carried over to locks and condition variables since they are also 
    implemented with semaphores.

    In the case of condition variables, an additional `priority` attribute is stored within 
    the semaphore_elem struct, and the list of waiters are sorted by this new attribute. As a 
    result, when we pop from the front in cond_signal we guarantee that the highest priority 
    threads are woken up.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

    1. lock_acquire is called on current thread
    2. current thread checks if it has higher priority than the lock holder
    2. If it does
        2.1 Recursively donate priority to the lock holder (i.e. lock holder also donates to locks it's waiting on, etc.)
    3. Else
        3.1 continue
    
    By step 2.1, the donation is carried down to an arbitrary level because each thread T at 
    depth `i` will check which lock it is waiting on, and donate to T_(i+1). The donations 
    stop once thread T_(i+1) is seen to already have equal or greater priority than T_i.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

    1. The lock is removed from the list of locks current thread holds
    2. The donated priority for current thread is re-calculated, because it 
       could be holding on to locks that other high priority threads are waiting on
    3. The highest priority thread waiting on this lock is woken up via a call to 
       sema_up (as described in B3)
    4. Current thread yields, and as part of the yielding process if it still has 
       the highest priority (based on other locks it owns), it runs again

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

    A potential race condition happens when we're calculated donated priority. Since the 
    priority donation is nested to any arbitrary level, a timer interrupt could force us to
    quit during the donation process. This is circumvented by disabling interrupts until
    donations are completed. Otherwise, a low-priority thread with a lock that others are waiting on may
    never get the chance to run again :(

    Can't use a lock for this specific race condition because that could entail 
    future priority donations, taking us back to where we started.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

    Keeping the ready_list & waiters lists sorted at all times simplified how we queue up
    threads, and calls to list_insert_ordered kept things efficient too. The focus of our 
    implementation was to use as little code as possible while solving the problem at hand 
    while minimizing risk of introducing new bugs.

    Another possible solution we considered was keeping track of which threads donated what 
    amount, and creating a more intricate web of connections that any thread could see. 
    This ended up massively complicating our solution and, in the end, we determined that 
    it was unnecessary to keep given how much more computation, code and complexity would 
    be involved. Our philosophy tackling this was to be laser-focused on what a running 
    thread needs access to, and the least amount of information it can get away with.


              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

    struct thread
    {
        int recent_cpu;                     /* Fixed point value of thread cpu usage. */
        int nice_value;                     /* Indicates how willing a thread is to give up priority to another. Value between -20 and 20 */
    }

    // In fixed-point.h
    #define SHIFT_FACTOR 16384              /* Take last 14 bits in signed 32-bit integer as fractional component. */

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     B
12      8   4   0  61  60  59     A
16      12  4   0  60  60  59     B
20      12  8   0  60  59  59     A
24      16  8   0  59  59  59     C  
28      16  8   4  59  59  58     B
32      16  12  4  59  58  58     A
36      20  12  4  58  58  58     C

    We assume that load_avg is zero at all times, because it is calculated 
    every one second. Since only 36 timer ticks have elapsed the load_avg 
    was not recalculated as given in the scheduler specifications. 

    Further, the above assumes that priorities are calculated on a smaller 
    interval, because normally it would happen ever 100 timer ticks, so 
    in reality, thread A would keep running until 100. At least the way we 
    implemented it...

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

    The scheduler specification did not indicate whether the current running thread 
    should yield immediately after its new recent_cpu values gives it a lower 
    priority than the next waiting thread, since priorities are only compared 
    every 1 second (100 timer ticks). The rule we made to resolve this was to simply 
    wait until 1 second had passed for entries to be updated in ready_list.

    Another ambiguity was the order in which to calculate load_avg and recent_cpu. The 
    order is quite important because the recent_cpu could be artificially inflated, 
    and so in the end we decided to calculate recent_cpu first because it more 
    accurately reflected the test cases.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

    If the runtime complexity is too large in/outside interrupt contexts, then the running thread 
    might not have enough time to execute that task it would've. This could artificially inflate 
    the load_avg value because the thread would need to get re-queued by the scheduler at a later 
    time, thereby keeping the ready_list size larger than it needs to be for a longer period of time.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

    The design is pretty straight forward, but the obvious critic would be 
    the new fixed-point.c file. The reference solution only introduced a 
    single new header file, and so a more compact design could make catching 
    bugs easier. The advantage over hard-coding the fixed-point operations 
    was that it can be re-used as many times as desired and keeps the code 
    more easily maintainable. 

    Also, by keeping track of the recent_cpu and nice values as part of each 
    thread and not in some global array we avoided running into issues with 
    maintaining such a data structure, such has having to re-size it 
    and clean out values for dead/dying threads.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

    Similar answer to the previous question, instead of writing giant formulas 
    into a single/multiple lines, breaking down each bit of functionality 
    increased re-usability and prevented me from completely pulling my hair out. 
    Further, during the development process there were many small changes made to 
    the fixed-point math, and so keeping it abstracted away allowed for more 
    rapid development & testing. 


               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

	One of the few difficulties about this assignment for us was getting up to 
	speed and comfortable with how the pintos code worked. There is a lot of 
	documentation and it felt a bit overwhelming at first. Because of the open 
	eneded nature of the tasks, it required thinking up a few different solutions 
	to the same problem, and choosing one of them to be the way you are going to 
	implement your code. We spent the most time not on writing the actual code 
	itself, but instead on debugging small issues that arrised from that process. 
	I'm glad we had tests to check against because otherwise we wouldn't really have
	any indication that we were introducing bugs into the code due to the nature of
	the concurrency of the code and the degree of coupling of all the systems. 

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

	Understanding how something works from reading about it is one thing, but
	actually having the opportunity to put it into practice brings that understanding
	to a next level. Writing the code for the different parts of this assignment
	definitely gave us a good look into what real operating systems have to deal with
	(on a more abstract level of course). It's true about other courses as well, but
	it's especially true here that you get out of the assignment proportionally what
	you put into it.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

	Calculating the load_avg and recent_cpu was a headache for us because to us it
	wasn't entirely clear exactly when and where we should be doing these calculations.
	The documentation mentions things need to be updated each second but other than
	that, it was a combination of reading through everything we could find in the pintos
	docs and piazza to get the intended behaviour. In hindsight it's easy to see that it
	really couldn't have been implemented another way, but from the prospective of us
	starting that task, we're not sure it was super clear.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
